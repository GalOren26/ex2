{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nm50ivzGvhM",
        "outputId": "bea46305-8163-4f1a-fbb2-a7a9a1cf17da"
      },
      "source": [
        "! git clone https://github.com/galoren287199/ex2.git\n",
        "%cd ex2 \n",
        "%ls\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ex2'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 12 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "/content/ex2\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  ex2.ipynb  lm.py  ptb-lm.py  reader.py  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BfXAKsqGnlj"
      },
      "source": [
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "def read_words(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        "\n",
        "\n",
        "def build_vocab(filename):\n",
        "  data = read_words(filename)\n",
        "  words=set(data)\n",
        "  word_to_id = dict(zip(words, range(len(words))))\n",
        "  id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
        "  return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def file_to_word_ids(filename, word_to_id):\n",
        "  data = read_words(filename)\n",
        "  return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "\n",
        "def files_raw_data(paths,names,word_to_id,trainRatio=1):\n",
        "    data={}\n",
        "    for idx,path in enumerate(paths):\n",
        "        data[names[idx]]=file_to_word_ids(path,word_to_id)\n",
        "    if trainRatio!=1: \n",
        "        data['train']=RandomSampleAcordionRatio(data['train'],trainRatio)\n",
        "    return data     \n",
        "def RandomSampleAcordionRatio(data,ratio):\n",
        "     start_idx=int(np.random.uniform(0,1-ratio)*len(data))\n",
        "     end_idx=start_idx+int(ratio*len(data))\n",
        "     return data[start_idx:end_idx]\n",
        "def full_path_files(base,names_of_files):\n",
        "    return [base+name for name in names_of_files]\n",
        "\n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "       \n",
        "  \"\"\"Iterate on the raw PTB data.\n",
        "  This generates batch_size pointers into the raw PTB data, and allows\n",
        "  minibatch iteration along these pointers. \n",
        "  \n",
        "  make the raw data iterater in dim  [batch_size,seq_len] when all the data store at \n",
        "  [batch_size,batch_len]-> batch_len*batch*size=len(data)\n",
        "  \n",
        "  Args:\n",
        "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
        "    batch_size: int, the batch size.\n",
        "    num_steps: int, the number of unrolls.\n",
        "  Yields:\n",
        "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
        "    The second element of the tuple is the same data time-shifted to the\n",
        "    right by one.\n",
        "  Raises:\n",
        "    ValueError: if batch_size or num_steps are too high.\n",
        "  \"\"\"\n",
        "  \n",
        "  raw_data = np.array(raw_data, dtype=np.int32)\n",
        "  data_len = len(raw_data)\n",
        "  batch_len = data_len // batch_size\n",
        "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "  for i in range(batch_size):\n",
        "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        "\n",
        "\n",
        "  epoch_size = (batch_len - 1) // num_steps\n",
        "\n",
        "  if epoch_size == 0:\n",
        "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "\n",
        "  for i in range(epoch_size):\n",
        "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "    yield (x, y)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77kfz4PGGlOp"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.tensor as tensor \n",
        "class Rnn(nn.Module):\n",
        "  \"\"\"Simple LSMT-based language model\"\"\"\n",
        "  \n",
        "  #to change embbeding dim \n",
        "  #to change drop out  val \n",
        "   # change init_hidden \n",
        "   # check the lerning rate\n",
        "   #change  dp_keep_prob\n",
        "   \n",
        "   # consider of we need clip_grad_norm\n",
        "   #consider using step insted \n",
        "   \n",
        "   #CHECK FOR PREPLXTIY CALC \n",
        "   \n",
        "  def __init__(self,net,embedding_dim, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob,lr=1,lr_decay_base=2,ephocs_witout_decay=4):\n",
        "    super(Rnn, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.seq_len = seq_len\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dp_keep_prob = dp_keep_prob\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
        "    self.net_name=net\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    if net==\"lstm\":\n",
        "        self.net = nn.LSTM(input_size=embedding_dim,\n",
        "                                hidden_size=embedding_dim,\n",
        "                                num_layers=num_layers,\n",
        "                                dropout=1 - dp_keep_prob)\n",
        "    \n",
        "    else:\n",
        "        self.net =nn.GRU(input_size=embedding_dim,\n",
        "                                hidden_size=embedding_dim,\n",
        "                                num_layers=num_layers,\n",
        "                                dropout=1 - dp_keep_prob)\n",
        "    self.sm_fc = nn.Linear(in_features=embedding_dim,\n",
        "                           out_features=vocab_size)\n",
        "    # self.lr=nn.Parameter(tensor(lr),requires_grad=False)\n",
        "    # self.ephocs_witout_decay=nn.Parameter(tensor(ephocs_witout_decay),requires_grad=False)\n",
        "    # self.lr_decay_base=nn.Parameter(tensor(lr_decay_base),requires_grad=False)\n",
        "    self.lr=lr\n",
        "    self.ephocs_witout_decay=ephocs_witout_decay\n",
        "    self.lr_decay_base=lr_decay_base\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_range = 0.1\n",
        "    nn.init.xavier_normal_(self.word_embeddings.weight.data)\n",
        "    # self.word_embeddings.weight.data.uniform_(-init_range, init_range)\n",
        "    self.sm_fc.bias.data.fill_(0.0)\n",
        "    nn.init.xavier_normal_(self.sm_fc.weight.data)\n",
        "    \n",
        "    # self.sm_fc.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "  def init_hidden(self):\n",
        "    weight = next(self.parameters()).data\n",
        "    if self.net_name==\"lstm\":\n",
        "        return (Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()),\n",
        "            Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()))\n",
        "    else :\n",
        "        return   Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_())\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    if self.dp_keep_prob==1:\n",
        "      embeds=self.word_embeddings(inputs)\n",
        "    else : \n",
        "      embeds = self.dropout(self.word_embeddings(inputs))\n",
        "    net_out, hidden = self.net(embeds, hidden)\n",
        "    net_out = self.dropout(net_out)\n",
        "    logits = self.sm_fc(net_out.view(-1, self.embedding_dim))\n",
        "    return logits.view(self.seq_len, self.batch_size, self.vocab_size), hidden\n",
        "\n",
        "# def repackage_hidden(h):\n",
        "#        \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "#         if isinstance(h, torch.Tensor):\n",
        "#             return h.detach()\n",
        "#         else:\n",
        "#             return tuple(repackage_hidden(v) for v in h)\n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "  if type(h) is  not tuple:\n",
        "    return Variable(h.data)\n",
        "  # else:\n",
        "  #  return tuple(repackage_hidden(v) for v in h)\n",
        "  else:\n",
        "      temp=[]\n",
        "      for i in h:\n",
        "          temp.append(Variable(i.data))\n",
        "      return tuple(temp)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "MxBm_MqLyhPn",
        "outputId": "571d5a9c-b002-4ac5-c36a-88fcb438cc5c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.tensor as tensor \n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import logging\n",
        "\n",
        "args={\"seq_len\":25,\"batch_size\":20,\"inital_lr\":10,\"num_epochs\":16,  \"lr_decay_base\":1.7,\"ephocs_witout_decay\":4\n",
        "         ,\"trainRatio\":1,\"data\":\"data/\", \"hidden_size\":200 ,\"num_layers\":2, \"dp_keep_prob\":1,  \"net\":\"gru \", \"load\":\"\",\"save\":\"lm_model2.pt\", \"log_path\":\"./log_gru_DO.txt\" }\n",
        "\n",
        "\n",
        "def save_checkpoint(model,save_path, epoch,trained):\n",
        "    torch.save({\n",
        "        'model': model,\n",
        "        'epoch': epoch,\n",
        "        'trained':trained\n",
        "    }, save_path)\n",
        "def load_checkpoint(load_path):\n",
        "    \n",
        "    checkpoint = torch.load(load_path)\n",
        "    model=checkpoint['model']\n",
        "    epoch = checkpoint['epoch']\n",
        "    trained=checkpoint['trained']\n",
        "    if trained:\n",
        "        print('continue to train from epoch {}, see log file for history :)\\n'.format(epoch))\n",
        "    else:\n",
        "        print('load model from file! validate on valid data :)\\n')\n",
        "    return model,epoch ,trained\n",
        "\n",
        "    \n",
        "def run_epoch(model, data, is_train=False, lr=1.0):\n",
        "  \"\"\"Runs the model on the given data.\"\"\"\n",
        "  if is_train:\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()\n",
        "    \n",
        "  num_of_seq = ((len(data) // model.batch_size) - 1) // model.seq_len\n",
        "  start_time = time.time()\n",
        "  hidden = model.init_hidden()\n",
        "  costs = 0.0\n",
        "  iters = 0\n",
        "  for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n",
        "    inputs = Variable(torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous()).to(device)\n",
        "    model.zero_grad()\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    outputs, hidden = model(inputs, hidden)\n",
        "    targets = Variable(torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous()).to(device)\n",
        "    tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n",
        "\n",
        "    loss = criterion(outputs.view(-1, model.vocab_size), tt)\n",
        "    costs += loss.item() * model.seq_len\n",
        "    iters += model.seq_len\n",
        "\n",
        "    if is_train:\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "      for p in model.parameters():\n",
        "        p.data.add_(-lr, p.grad.data)\n",
        "      # if step % (num_of_seq // 10) == 10:\n",
        "      #   print(\"{} perplexity: {:8.2f} speed: {} wps\".format(step * 1.0 / num_of_seq, np.exp(costs / iters),\n",
        "      #                                                  iters * model.batch_size / (time.time() - start_time)))\n",
        "  return np.exp(costs / iters)    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model=None,epoch=None):\n",
        "    #load model or create it     \n",
        "    if model!=None:\n",
        "        model,epoch_num=model,epoch\n",
        "    else:      \n",
        "        epoch_num=args[\"num_epochs\"]\n",
        "        model = Rnn(net=args[\"net\"],embedding_dim=200,seq_len=args[\"seq_len\"], batch_size=args[\"batch_size\"],\n",
        "                 vocab_size=vocab_size, num_layers=args[\"num_layers\"], dp_keep_prob=args[\"dp_keep_prob\"],lr=args[\"inital_lr\"],lr_decay_base=args[\"lr_decay_base\"],ephocs_witout_decay=args[\"ephocs_witout_decay\"])\n",
        "     #convert to device \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    # if load is avilable get lr and num of ephoc else take the defualt. \n",
        "    cur_epoch=0\n",
        "    \n",
        "    prep_test_list=[]\n",
        "    f = open(args[\"log_path\"], \"a\")\n",
        "    try:\n",
        "        for epoch in range(epoch_num):\n",
        "          cur_epoch=epoch\n",
        "          if ( epoch>model.ephocs_witout_decay):\n",
        "                model.lr=model.lr/model.lr_decay_base\n",
        "          train_perplexity = run_epoch(model, data[\"train\"], True, model.lr)\n",
        "          test_perplexity=run_epoch(model, data[\"test\"])\n",
        "    \n",
        "          if len(prep_test_list)<5:\n",
        "            prep_test_list.append(test_perplexity)\n",
        "                \n",
        "          elif test_perplexity>max(prep_test_list) :\n",
        "                min_val=min(prep_test_list)\n",
        "                f.write('test perplexity at best epoch  {}'.format(min_val))\n",
        "                f.write('test perplexity at last ephoc  {}: {:8.2f}'.format(epoch,test_perplexity))\n",
        "                f.write(\"\".join([\"    {}: {}\".format(key, value) for key, value in trial.params.items()])) # to change later \n",
        "                f.write(\"\\n\\n\")\n",
        "                return test_perplexity\n",
        "          else:\n",
        "              max_key=max(prep_test_list)\n",
        "              prep_test_list[prep_test_list.index(max_key)]=test_perplexity\n",
        "              if min(prep_test_list)==test_perplexity:\n",
        "                  print(\"save model new result :)\\n\")\n",
        "                  save_checkpoint(model,args[\"save\"],epoch,True)\n",
        "              avg=sum(prep_test_list)/len(prep_test_list)\n",
        "              bool2=100*(abs((test_perplexity-avg))/avg)<2\n",
        "              if bool2:\n",
        "                model.lr*=model.lr_decay_base\n",
        "            \n",
        "          print('Train perplexity at epoch {}: {:8.2f}'.format(epoch, train_perplexity))\n",
        "          print('test perplexity at epoch {}: {:8.2f}'.format(epoch, test_perplexity))\n",
        "          f.write('Train train train train train train  perplexity at epoch {}: {:8.2f}\\n'.format(epoch, train_perplexity))\n",
        "          f.write('test perplexity at epoch {}: {:8.2f}\\n\\n'.format(epoch, test_perplexity))\n",
        "            \n",
        "  \n",
        "        min_val=min(prep_test_list)\n",
        "        f.write('test perplexity at best epoch {}'.format(min_val))\n",
        "        return test_perplexity,model\n",
        "    except: \n",
        "        print(\"problem or stop by user ,saving current model state fir continue later! \")\n",
        "        save_checkpoint(model,args[\"save\"],cur_epoch,False)\n",
        "def PlotResult():\n",
        "            print(\"test max precision \",self.highest_test_acc,\"%\" )\n",
        "            print(\"Script Runtime:\",(time.time()-self.T),\"Seconds\")\n",
        "            f1 = plt.figure()\n",
        "            plt.plot(range(len(self.train_accuracy_array)),self.train_accuracy_array)\n",
        "            plt.plot(range(len(self.test_accuracy_array)),self.test_accuracy_array)   \n",
        "            plt.legend([\"train acc\", \"test acc\"], loc =\"lower right\")\n",
        "            plt.xlabel(\"Epoch #\")\n",
        "            plt.ylabel(\"Accuracy[%]\")\n",
        "            plt.title(self.reg_method)\n",
        "            plt.show()\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #open log file resdults \n",
        "    f = open(args[\"log_path\"], \"w\")\n",
        "    f.write(\"hello from traing with Optuna\\n\")\n",
        "    f.close()\n",
        "    ## --read data and parse to voacb \n",
        "    paths=[\"ptb.train.txt\",\"ptb.valid.txt\",\"ptb.test.txt\"]\n",
        "    names_of_files=[\"train\",\"valid\",\"test\"]\n",
        "    paths = full_path_files(args[\"data\"],paths)\n",
        "    train_name=paths[0]\n",
        "    word_to_id, id_2_word =build_vocab(train_name)\n",
        "    # return for each file list of the words in the file encoded to id by the word to id dict \n",
        "    data=files_raw_data(paths,names_of_files,word_to_id,args[\"trainRatio\"])\n",
        "    vocab_size = len(word_to_id)\n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    print (\"------- training ------\\n\")\n",
        "    if args['load']!=\"\":\n",
        "        model,epoch,trained=load_checkpoint(args['load'])\n",
        "        test_perplexity,model=train(model,epoch)\n",
        "    else :\n",
        "        test_perplexity,model=train() \n",
        "        \n",
        "    print (\"---- end training ------\\n\")\n",
        "    print(\"---  Validating ---\")\n",
        "    model.batch_size = 1 # to make sure we process all the data\n",
        "    print('Validation Perplexity: {:8.2f}'.format(run_epoch(model,  data[\"valid\"])))\n",
        "    \n",
        "    \n",
        "    print(\"----Done! ---\")\n",
        "   \n",
        "    \n",
        "   \n",
        "    \n",
        "# get hyper params with optuna     \n",
        "    \n",
        "\n",
        "# study = optuna.create_study( direction='minimize', pruner=optuna.pruners.MedianPruner(\n",
        "#     n_startup_trials=5, n_warmup_steps=7, interval_steps=1)\n",
        "# )\n",
        "# study.optimize(train, n_trials=70)\n",
        "# plot_optimization_history(study)\n",
        "\n",
        "# print(\"Study statistics: \")\n",
        "# print(\"  Number of finished trials: \", len(study.trials))\n",
        "# print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "# print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print(\"  Value: \", trial.value)\n",
        "\n",
        "# print(\"  Params: \")\n",
        "# for key, value in trial.params.items():\n",
        "#     print(\"    {}: {}\".format(key, value))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------- training ------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "problem or stop by user ,saving current model state fir continue later! \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b287d387baee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m           \u001b[0mtrain_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m           \u001b[0mtest_perplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b287d387baee>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, data, is_train, lr)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b287d387baee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mtest_perplexity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtest_perplexity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"---- end training ------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b287d387baee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"problem or stop by user ,saving current model state fir continue later! \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"save\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPlotResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test max precision \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighest_test_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b287d387baee>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(model, save_path, epoch, trained)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m'trained'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     }, save_path)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:274] . unexpected pos 3008 vs 2942"
          ]
        }
      ]
    }
  ]
}